@misc{p301,
    title = {{Youtube Company Statistics}},
    year = {2017},
    howpublished = "\url{http://www.statisticbrain.com/youtube-statistics/}"
}

@article{p302,
    title = "Big Data computing and clouds: Trends and future directions ",
    author = "Roberto V Zicari",
    number = "",
    pages = "103 - 128",
    year = "2013",
}

@inproceedings{p303,
 author = {Cox, Michael and Ellsworth, David},
 title = {Application-controlled Demand Paging for Out-of-core Visualization},
 booktitle = {Proceedings of the 8th Conference on Visualization '97},
 series = {VIS '97},
 year = {1997},
 isbn = {1-58113-011-2},
 location = {Phoenix, Arizona, USA},
 pages = {235--ff.},
 url = {http://dl.acm.org/citation.cfm?id=266989.267068},
 acmid = {267068},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
 keywords = {computational fluid dynamics, out-of-core visualization, visualization},
} 

@article{p304,
  author    = {Jonathan Stuart Ward and
               Adam Barker},
  title     = {Undefined By Data: {A} Survey of Big Data Definitions},
  journal   = {CoRR},
  volume    = {abs/1309.5821},
  year      = {2013},
  url       = {http://arxiv.org/abs/1309.5821},
  timestamp = {Wed, 07 Jun 2017 14:42:29 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/WardB13a},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@techreport{p305,
  added-at = {2013-03-15T10:26:13.000+0100},
  author = {Laney, Douglas},
  biburl = {https://www.bibsonomy.org/bibtex/263868097d6e1998de3d88fcbb7670ca6/sb3000},
  institution = {META Group},
  interhash = {742811cb00b303261f79a98e9b80bf49},
  intrahash = {63868097d6e1998de3d88fcbb7670ca6},
  keywords = {3v analyst-report bigdata},
  month = {02},
  timestamp = {2013-03-15T10:26:13.000+0100},
  title = {{3D} Data Management: Controlling Data Volume, Velocity, and Variety},
  url = {http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf},
  year = 2001
}

@inproceedings{p306,
 author = {Babcock, Brian and Babu, Shivnath and Datar, Mayur and Motwani, Rajeev and Widom, Jennifer},
 title = {Models and Issues in Data Stream Systems},
 booktitle = {Proceedings of the Twenty-first ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
 series = {PODS '02},
 year = {2002},
 isbn = {1-58113-507-6},
 location = {Madison, Wisconsin},
 pages = {1--16},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/543613.543615},
 doi = {10.1145/543613.543615},
 acmid = {543615},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@Article{p307,
author="ur Rehman, Muhammad Habib
and Liew, Chee Sun
and Abbas, Assad
and Jayaraman, Prem Prakash
and Wah, Teh Ying
and Khan, Samee U.",
title="Big Data Reduction Methods: A Survey",
journal="Data Science and Engineering",
year="2016",
month="12",
day="01",
volume="1",
number="4",
pages="265--284",
abstract="Research on big data analytics is entering in the new phase called fast data where multiple gigabytes of data arrive in the big data systems every second. Modern big data systems collect inherently complex data streams due to the volume, velocity, value, variety, variability, and veracity in the acquired data and consequently give rise to the 6Vs of big data. The reduced and relevant data streams are perceived to be more useful than collecting raw, redundant, inconsistent, and noisy data. Another perspective for big data reduction is that the million variables big datasets cause the curse of dimensionality which requires unbounded computational resources to uncover actionable knowledge patterns. This article presents a review of methods that are used for big data reduction. It also presents a detailed taxonomic discussion of big data reduction methods including the network theory, big data compression, dimension reduction, redundancy elimination, data mining, and machine learning methods. In addition, the open research issues pertinent to the big data reduction are also highlighted.",
issn="2364-1541",
doi="10.1007/s41019-016-0022-0",
url="https://doi.org/10.1007/s41019-016-0022-0"
}

@article{thesis1,
 author = {Yan, Jun and Zhang, Benyu and Liu, Ning and Yan, Shuicheng and Cheng, Qiansheng and Fan, Weiguo and Yang, Qiang and Xi, Wensi and Chen, Zheng},
 title = {Effective and Efficient Dimensionality Reduction for Large-Scale and Streaming Data Preprocessing},
 journal = {IEEE Trans. on Knowl. and Data Eng.},
volume = {18},
 number = {3},
 month = mar,
 year = {2006},
 issn = {1041-4347},
 pages = {320--333},
 numpages = {14},
 url = {http://dx.doi.org/10.1109/TKDE.2006.45},
 doi = {10.1109/TKDE.2006.45},
 acmid = {1112782},
 publisher = {IEEE Educational Activities Department},
 address = {Piscataway, NJ, USA},
 keywords = {Index Terms- Feature extraction, Index Terms- Feature extraction, feature selection, orthogonal centroid algorithm., feature selection, orthogonal centroid algorithm.},
} 
@article{thesis2,
author={Jieping Ye and Qi Li and Hui Xiong and H. Park and R. Janardan and V. Kumar},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={IDR/QR: an incremental dimension reduction algorithm via QR decomposition},
year={2005},
volume={17},
number={9},
pages={1208-1222},
keywords={data mining;learning (artificial intelligence);pattern classification;singular value decomposition;statistical analysis;very large databases;IDR;LDA-based incremental dimension reduction algorithm;QR decomposition;QR-updating technique;classification error rate;data mining;data preprocessing;eigenvalue problem;incremental learning;linear discriminant analysis;scatter matrices;singular value decomposition;Algorithm design and analysis;Computational efficiency;Data mining;Data preprocessing;Databases;Error analysis;Information retrieval;Linear discriminant analysis;Matrix decomposition;Singular value decomposition;Index Terms- Dimension reduction;QR Decomposition;Singular Value Decomposition (SVD).;incremental learning;linear discriminant analysis},
doi={10.1109/TKDE.2005.148},
ISSN={1041-4347},
month=Sep,
}
@INPROCEEDINGS{thesis4,
author={Y. Li and L. Q. Xu and J. Morphett and R. Jacobs},
booktitle={Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429)},
title={An integrated algorithm of incremental and robust PCA},
year={2003},
volume={1},
keywords={image processing;pattern recognition;principal component analysis;PCA;image processing;integrated algorithm;pattern recognition;principal component analysis;Covariance matrix;Eigenvalues and eigenfunctions;Image coding;Image processing;Image recognition;Jacobian matrices;Large-scale systems;Optimization methods;Principal component analysis;Robustness},
doi={10.1109/ICIP.2003.1246944},
ISSN={1522-4880},
month=Sep,
}
@article{thesis5,
author={Juyang Weng and Yilu Zhang and Wey-Shiuan Hwang},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Candid covariance-free incremental principal component analysis},
year={2003},
volume={25},
number={8},
pages={1034-1040},
keywords={Gaussian distribution;covariance matrices;eigenvalues and eigenfunctions;image processing;principal component analysis;real-time systems;Gaussian distribution;appearance-based image analysis techniques;candid covariance-free IPCA;candid covariance-free incremental principal component analysis;cerebral cortex;covariance matrix;eigenvector;high-dimensional image vectors;image analysis;mean of observations;real-time applications;statistical efficiency;Biology computing;Covariance matrix;Distributed computing;Filters;Humans;Image converters;Image sequence analysis;Image storage;Pixel;Principal component analysis},
doi={10.1109/TPAMI.2003.1217609},
ISSN={0162-8828},
month=Aug,
}
@article{1thesis2,
author={Shaoning Pang and S. Ozawa and N. Kasabov},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
title={Incremental linear discriminant analysis for classification of data streams},
year={2005},
volume={35},
number={5},
pages={905-914},
keywords={feature extraction;learning (artificial intelligence);pattern classification;principal component analysis;data stream classification;discriminant eigenspace;feature extraction;incremental linear discriminant analysis;incremental principle component analysis;pattern recognition;Covariance matrix;Data mining;Face recognition;Feature extraction;Linear discriminant analysis;Mobile robots;Pattern analysis;Pattern recognition;Principal component analysis;Testing;Classification;data stream;incremental linear discriminant analysis;incremental principle component analysis;linear discriminant analysis;pattern recognition;principle component analysis;Algorithms;Artificial Intelligence;Cluster Analysis;Computer Simulation;Discriminant Analysis;Image Enhancement;Information Storage and Retrieval;Linear Models;Models, Statistical;Pattern Recognition, Automated;Signal Processing, Computer-Assisted},
doi={10.1109/TSMCB.2005.847744},
ISSN={1083-4419},
month=Oct,
}

@INPROCEEDINGS{1thesis3,
author={N. Djuric and S. Vucetic},
booktitle={2013 IEEE 13th International Conference on Data Mining},
title={Efficient Visualization of Large-Scale Data Tables through Reordering and Entropy Minimization},
year={2013},
pages={121-130},
keywords={computational complexity;data visualisation;entropy;financial data processing;traffic engineering computing;travelling salesman problems;EM-ordering;TSP heuristic;data distribution;entropy minimization;financial data set analysis;heat map;information-theoretic approach;large-scale data set visual exploration;large-scale data table visualization;ordered data table;predictive coding residual;real-world traffic analysis;reordering;rescaling columns;time complexity;traveling salesman problem;Cities and towns;Clustering algorithms;Data visualization;Entropy;Heating;Minimization;Principal component analysis;data reordering;data seriation;data visualization;heatmap;large-scale data;traveling salesman problem},
doi={10.1109/ICDM.2013.63},
ISSN={1550-4786},
month=Dec,
}
@INPROCEEDINGS{thesis3,
author={M. Ye and X. Li and M. E. Orlowska},
booktitle={Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)},
title={Supervised Dimensionality Reduction on Streaming Data},
year={2007},
volume={1},
pages={674-678},
keywords={data analysis;learning (artificial intelligence);linear discriminant analysis;streaming data;supervised dimensionality reduction;Australia;Computer science;Covariance matrix;Data analysis;Data engineering;Eigenvalues and eigenfunctions;Information analysis;Information technology;Linear discriminant analysis;Matrix decomposition},
doi={10.1109/FSKD.2007.548},
month=Aug,
}
@Inbook{thesis6,
author="Laaksonen, Jorma
and Oja, Erkki",
editor="von der Malsburg, Christoph
and von Seelen, Werner
and Vorbr{\"u}ggen, Jan C.
and Sendhoff, Bernhard",
title="Subspace dimension selection and averaged learning subspace method in handwritten digit classification",
bookTitle="Artificial Neural Networks --- ICANN 96: 1996 International Conference Bochum, Germany, July 16--19, 1996 Proceedings",
year="1996",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="227--232",
abstract="We present recent improvements in using subspace classifiers in recognition of handwritten digits. Both non-trainable CLAFIC and trainable ALSM methods are used with four models for initial selection of subspace dimensions and their further error-driven refinement. The results indicate that these additions to the subspace classification scheme noticeably reduce the classification error.",
isbn="978-3-540-68684-2",
doi={10.1007/3-540-61510-5_41},
url="https://doi.org/10.1007/3-540-61510-5_41",
}

@article{thesis7,
title = "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix",
journal = "Journal of Mathematical Analysis and Applications",
volume = "106",
number = "1",
pages = "69 - 84",
year = "1985",
note = "",
issn = "0022-247X",
doi = "http://dx.doi.org/10.1016/0022-247X(85)90131-3",
url = "http://www.sciencedirect.com/science/article/pii/0022247X85901313",
author = "Erkki Oja and Juha Karhunen",
}

@article{thesis8,
title = "Optimal unsupervised learning in a single-layer linear feedforward neural network",
journal = "Neural Networks",
volume = "2",
number = "6",
pages = "459 - 473",
year = "1989",
note = "",
issn = "0893-6080",
doi = "http://dx.doi.org/10.1016/0893-6080(89)90044-0",
url = "http://www.sciencedirect.com/science/article/pii/0893608089900440",
author = "Terence D. Sanger",
keywords = "Neural network",
keywords = "Unsupervised learning",
keywords = "Hebbian learning",
keywords = "Feedforward",
keywords = "Karhunen-Loeve Transform",
keywords = "Image coding",
keywords = "Texture",
keywords = "Cortical receptive fields",
}

@INPROCEEDINGS{thesis9,
    author = {Peter M. Hall and David Marshall and Ralph R. Martin},
    title = {Incremental Eigenanalysis for Classification},
    booktitle = {in British Machine Vision Conference},
    year = {1998},
    pages = {286--295},
}

@article{1thesis0,
title = "An Eigenspace Update Algorithm for Image Analysis",
journal = "Graphical Models and Image Processing",
volume = "59",
number = "5",
pages = "321 - 332",
year = "1997",
note = "",
issn = "1077-3169",
doi = "http://dx.doi.org/10.1006/gmip.1997.0425",
url = "http://www.sciencedirect.com/science/article/pii/S1077316997904251",
author = "S. Chandrasekaran and B.S. Manjunath and Y.F. Wang and J. Winkeler and H. Zhang",

}

@ARTICLE{1thesis1,
author={R. D. DeGroat and R. A. Roberts},
journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},
title={Efficient, numerically stabilized rank-one eigenstructure updating [signal processing]},
year={1990},
volume={38},
number={2},
pages={301-316},
keywords={eigenvalues and eigenfunctions;iterative methods;signal processing;eigenvalue decomposition;eigenvalue iteration;numerical stabilization technique;online computation;rank-one eigenstructure updating;roundoff error buildup;signal processing;time-varying subspaces;Acoustic signal processing;Array signal processing;Covariance matrix;Direction of arrival estimation;Eigenvalues and eigenfunctions;Sensor arrays;Signal processing;Signal processing algorithms;Signal to noise ratio;Very large scale integration},
doi={10.1109/29.103066},
ISSN={0096-3518},
month=Feb,
}
@article{1thesis4,
author = {Eisen, Michael B. and Spellman, Paul T. and Brown, Patrick O. and Botstein, David}, 
title = {Cluster analysis and display of genome-wide expression patterns},
volume = {95}, 
number = {25}, 
pages = {14863-14868}, 
year = {1998}, 
abstract ={A system of cluster analysis for genome-wide expression data from DNA microarray hybridization is described that uses standard statistical algorithms to arrange genes according to similarity in pattern of gene expression. The output is displayed graphically, conveying the clustering and the underlying expression data simultaneously in a form intuitive for biologists. We have found in the budding yeast Saccharomyces cerevisiae that clustering gene expression data groups together efficiently genes of known similar function, and we find a similar tendency in human data. Thus patterns seen in genome-wide expression experiments can be interpreted as indications of the status of cellular processes. Also, coexpression of genes of known function with poorly characterized or novel genes may provide a simple means of gaining leads to the functions of many genes for which information is not available currently.}, 
URL = {http://www.pnas.org/content/95/25/14863.abstract}, 
eprint = {http://www.pnas.org/content/95/25/14863.full.pdf}, 
journal = {Proceedings of the National Academy of Sciences} 
}

@inproceedings{1thesis5,
 author = {Ding, Chris and He, Xiaofeng},
 title = {Linearized Cluster Assignment via Spectral Ordering},
 booktitle = {Proceedings of the Twenty-first International Conference on Machine Learning},
 series = {ICML '04},
 year = {2004},
 isbn = {1-58113-838-5},
 location = {Banff, Alberta, Canada},
 pages = {30--},
 url = {http://doi.acm.org/10.1145/1015330.1015407},
 doi = {10.1145/1015330.1015407},
 acmid = {1015407},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{1thesis6,
 author = {M.Sharma, N.S.Yadav},
 title = {A Practical Approach to Process Streaming Data using Graph Database},
 volume = {117},
 number = {23},
 year = {2015},
 journal = {International Journal of Computer Applications}
}

@misc{1thesis7,
 title = {What is Streaming Data?},
 year = {2017},
 howpublished = "\url{https://aws.amazon.com/streaming-data/}"
} 

@Inbook{1thesis8,
author="Ortiz Laguna, Javier
and Olaya, Angel Garc{\'i}a
and Borrajo, Daniel",
editor="Konstan, Joseph A.
and Conejo, Ricardo
and Marzo, Jos{\'e} L.
and Oliver, Nuria",
title="A Dynamic Sliding Window Approach for Activity Recognition",
bookTitle="User Modeling, Adaption and Personalization: 19th International Conference, UMAP 2011, Girona, Spain, July 11-15, 2011. Proceedings",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="219--230",
abstract="Human activity recognition aims to infer the actions of one or more persons from a set of observations captured by sensors. Usually, this is performed by following a fixed length sliding window approach for the features extraction where two parameters have to be fixed: the size of the window and the shift. In this paper we propose a different approach using dynamic windows based on events. Our approach adjusts dynamically the window size and the shift at every step. Using our approach we have generated a model to compare both approaches. Experiments with public datasets show that our method, employing simpler models, is able to accurately recognize the activities, using fewer instances, and obtains better results than the approaches used by the datasets authors.",
isbn="978-3-642-22362-4",
doi="10.1007/978-3-642-22362-4_19",
url="https://doi.org/10.1007/978-3-642-22362-4_19"
}

@article{p308,
  author = {Michael Hahsler and Kurt Hornik and Christian Buchta},
  title = {Getting Things in Order: An Introduction to the {R} 
      Package seriation},
  journal = {Journal of Statistical Software},
  year = {2008},
  volume = {25},
  pages = {1--34},
  number = {3},
  month = {03},
  abstract = {Seriation, i.e., finding a linear order for a set of objects
      given data and a loss or merit function, is a basic problem in data
          analysis.  Caused by the problem's combinatorial nature, it is hard
          to solve for all but very small sets.  Nevertheless, both exact
          solution methods and heuristics are available.  In this paper we
          present the package~seriation which provides the infrastructure for
          seriation with R.  The infrastructure comprises data structures to
          represent linear orders as permutation vectors, a wide array of
          seriation methods using a consistent interface, a method to calculate
          the value of various loss and merit functions, and several
          visualization techniques which build on seriation. To illustrate how
          easily the package can be applied for a variety of applications, a
          comprehensive collection of examples is presented.},
  issn = {1548-7660},
  pdf = {https://www.jstatsoft.org/index.php/jss/article/view/v025i03/v25i03.pdf},
  doi = {10.18637/jss.v025.i03},
  nopdf = {http://michael.hahsler.net/research/seriation_JSS2008/seriation.pdf},
  category = {seriation, visualization, optimization}
}

@article{p309,
title = "Niermann, S. (2005), {"}optimizing the ordering of tables with evolutionary computation,{"} the American statistician, 59, 41-46: Comment by Young, Liu, and Hawkins and reply [2] (multiple letters)",
author = "Young, {S. Stanley} and Li Liu and Hawkins, {Douglas M.} and Stefan Niermann",
year = "2005",
month = "11",
doi = "10.1198/000313005X72171",
volume = "59",
pages = "353--354",
journal = "American Statistician",
issn = "0003-1305",
publisher = "American Statistical Association",
number = "4",
}

@article{p310,
author = {Leland Wilkinson and Michael Friendly},
title = {The History of the Cluster Heat Map},
journal = {The American Statistician},
volume = {63},
number = {2},
pages = {179-184},
year  = {2009},
publisher = {Taylor & Francis},
doi = {10.1198/tas.2009.0033},
URL = { http://dx.doi.org/10.1198/tas.2009.0033},
eprint = { http://dx.doi.org/10.1198/tas.2009.0033}
}

@Inbook{p311,
author="Bar-Joseph, Ziv
and Demaine, Erik D.
and Gifford, David K.
and Hamel, Ang{\`e}le M.
and Jaakkola, Tommi S.
and Srebro, Nathan",
editor="Guig{\'o}, Roderic
and Gusfield, Dan",
title="K-ary Clustering with Optimal Leaf Ordering for Gene Expression Data",
bookTitle="Algorithms in Bioinformatics: Second International Workshop, WABI 2002 Rome, Italy, September 17--21, 2002 Proceedings",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="506--520",
abstract="A major challenge in gene expression analysis is effective data organization and visualization. One of the most popular tools for this task is hierarchical clustering. Hierarchical clustering allows a user to view relationships in scales ranging from single genes to large sets of genes, while at the same time providing a global view of the expression data. However, hierarchical clustering is very sensitive to noise, it usually lacks of a method to actually identify distinct clusters, and produces a large number of possible leaf orderings of the hierarchical clustering tree. In this paper we propose a new hierarchical clustering algorithm which reduces susceptibility to noise, permits up to k siblings to be directly related, and provides a single optimal order for the resulting tree. Our algorithm constructs a k-ary tree, where each node can have up to k children, and then optimally orders the leaves of that tree. By combining k clusters at each step our algorithm becomes more robust against noise. By optimally ordering the leaves of the tree we maintain the pairwise relationships that appear in the original method. Our k-ary construction algorithm runs in O(n                     3) regardless of k and our ordering algorithm runs in O(4k+o(k)                     n                     3). We present several examples that show that our k-ary clustering algorithm achieves results that are superior to the binary tree results.",
isbn="978-3-540-45784-8",
doi="10.1007/3-540-45784-4_39",
url="https://doi.org/10.1007/3-540-45784-4_39"
}

@inproceedings{p312,
 author = {M\"{a}kinen, Erkki and Siirtola, Harri},
 title = {Reordering the Reorderable Matrix As an Algorithmic Problem},
 booktitle = {Proceedings of the First International Conference on Theory and Application of Diagrams},
 series = {Diagrams '00},
 year = {2000},
 isbn = {3-540-67915-4},
 pages = {453--467},
 numpages = {15},
 url = {http://dl.acm.org/citation.cfm?id=645970.674914},
 acmid = {674914},
 publisher = {Springer-Verlag},
 address = {London, UK, UK},
} 

@book{p313,
 author = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
 title = {Pattern Classification (2Nd Edition)},
 year = {2000},
 isbn = {0471056693},
 publisher = {Wiley-Interscience},
} 

@article {p314,
author = {Abdi, Hervé and Williams, Lynne J.},
title = {Principal component analysis},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
volume = {2},
number = {4},
publisher = {John Wiley & Sons, Inc.},
issn = {1939-0068},
url = {http://dx.doi.org/10.1002/wics.101},
doi = {10.1002/wics.101},
pages = {433--459},
keywords = {singular and eigen value decomposition, bilinear decomposition, factor scores and loadings, RESS PRESS, multiple factor analysis},
year = {2010},
}

@INPROCEEDINGS{p315, 
author={M. Artac and M. Jogan and A. Leonardis}, 
booktitle={Object recognition supported by user interaction for service robots}, 
title={Incremental PCA for on-line visual learning and recognition}, 
year={2002}, 
volume={3}, 
number={}, 
pages={781-784 vol.3}, 
keywords={eigenvalues and eigenfunctions;image recognition;principal component analysis;eigenvectors;incremental principal component analysis;online visual learning;simultaneous learning;visual recognition;Educational programs;Image databases;Information science;Principal component analysis}, 
doi={10.1109/ICPR.2002.1048133}, 
ISSN={1051-4651}, 
month={},}

@ARTICLE{p316, 
author={D. Chu and L. Z. Liao and M. K. P. Ng and X. Wang}, 
journal={IEEE Transactions on Neural Networks and Learning Systems}, 
title={Incremental Linear Discriminant Analysis: A Fast Algorithm and Comparisons}, 
year={2015}, 
volume={26}, 
number={11}, 
pages={2716-2735}, 
keywords={computational complexity;matrix decomposition;statistical analysis;ILDA algorithm;LDA-QR;ULDA algorithm;computational complexity;data matrix;economic QR factorization;incremental linear discriminant analysis;lower triangular linear system;space complexity;uncorrelated LDA algorithm;Algorithm design and analysis;Computational complexity;Economics;Eigenvalues and eigenfunctions;Linear discriminant analysis;Linear systems;Training;Classification accuracy;computational complexity;incremental linear discriminant analysis (ILDA);linear discriminant analysis (LDA);linear discriminant analysis (LDA).}, 
doi={10.1109/TNNLS.2015.2391201}, 
ISSN={2162-237X}, 
month={11},}

@Article{p317,
author="Kim, Tae-Kyun
and Stenger, Bj{\"o}rn
and Kittler, Josef
and Cipolla, Roberto",
title="Incremental Linear Discriminant Analysis Using Sufficient Spanning Sets and Its Applications",
journal="International Journal of Computer Vision",
year="2011",
month="01",
day="01",
volume="91",
number="2",
pages="216--232",
abstract="This paper presents an incremental learning solution for Linear Discriminant Analysis (LDA) and its applications to object recognition problems. We apply the sufficient spanning set approximation in three steps i.e. update for the total scatter matrix, between-class scatter matrix and the projected data matrix, which leads an online solution which closely agrees with the batch solution in accuracy while significantly reducing the computational complexity. The algorithm yields an efficient solution to incremental LDA even when the number of classes as well as the set size is large. The incremental LDA method has been also shown useful for semi-supervised online learning. Label propagation is done by integrating the incremental LDA into an EM framework. The method has been demonstrated in the task of merging large datasets which were collected during MPEG standardization for face image retrieval, face authentication using the BANCA dataset, and object categorisation using the Caltech101 dataset.",
issn="1573-1405",
doi="10.1007/s11263-010-0381-3",
url="https://doi.org/10.1007/s11263-010-0381-3"
}

@inproceedings{p401,
  title={Fast Online Incremental Learning on Mixture Streaming Data.},
  author={Wang, Yi and Fan, Xin and Luo, Zhongxuan and Wang, Tianzhu and Min, Maomao and Luo, Jiebo},
  booktitle={AAAI},
  pages={2739--2745},
  year={2017}
}

@inproceedings{pf03,
booktitle = {Eurographics Conference on Visualization (EuroVis) - STARs},
editor = {R. Borgo and F. Ganovelli and I. Viola},
title = {{Visualizing High-Dimensional Data: Advances in the Past Decade}},
author = {Liu, Shusen and Maljovec, Dan and Wang, Bei and Bremer, Peer-Timo and Pascucci, Valerio},
year = {2015},
publisher = {The Eurographics Association},
DOI = {10.2312/eurovisstar.20151115}
}

@article{pf01,
	author = {Fisseha Gidey G. and Charles Awono Onana},
	title = {High Dimensional Data Visualization: Advances and Challenges},
	journal = {International Journal of Computer Applications},
	issue_date = {March 2017},
	volume = {162},
	number = {10},
	month = {Mar},
	year = {2017},
	issn = {0975-8887},
	pages = {23-27},
	numpages = {5},
	url = {http://www.ijcaonline.org/archives/volume162/number10/27280-2017913362},
	doi = {10.5120/ijca2017913362},
	publisher = {Foundation of Computer Science (FCS), NY, USA},
	address = {New York, USA}
}
